{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210957\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk \n",
    "import re\n",
    "import string\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Create DataFrame\n",
    "jokes_df = pd.read_csv(r\"reddit_dadjokes.csv\")\n",
    "\n",
    "jokes_df.drop([\"author\", \"url\", \"score\", \"date\"], axis = 1, inplace = True)\n",
    "jokes_df.drop_duplicates(\"joke\", inplace = True)\n",
    "\n",
    "# Display cleaned data information\n",
    "jokes_df.head()\n",
    "print(len(jokes_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210957\n"
     ]
    }
   ],
   "source": [
    "# cleaning the data\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "jokes_df['joke'] = jokes_df['joke'].apply(clean_text)\n",
    "\n",
    "jokes_df.head()\n",
    "print(len(jokes_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 29999\n",
      "Sample word-to-index mapping: {'PAD': 0, 'UNK': 1, 'EOS': 4, 'SOS': 3, 'a': 5, 'the': 6, 'i': 7, ',': 8, '?': 9, '.': 10}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# Constants\n",
    "UNK = 'UNK' \n",
    "PAD_TOKEN = 'PAD'\n",
    "EOS = 'EOS'\n",
    "SOS = 'SOS'\n",
    "VOCAB_SIZE = 30000  # Example vocabulary size\n",
    "\n",
    "# Initialize word tokenizer\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Function to build vocabulary\n",
    "def build_vocab_from_df(df, text_column, vocab_size):\n",
    "    words = []\n",
    "    for joke in df[text_column]:\n",
    "        tokens = word_tokenizer.tokenize(joke.lower())  # Use TreebankWordTokenizer\n",
    "        tokens.append(EOS)  # Add EOS token at the end\n",
    "        words.extend(tokens)  # Collect tokens\n",
    "    \n",
    "    # Calculate word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Get the most common words\n",
    "    most_common = word_freq.most_common(vocab_size - 4)\n",
    "    \n",
    "    # Build vocabulary with special tokens\n",
    "    vocab = [(PAD_TOKEN, 0), (UNK, 0), (EOS, 0), (SOS, 0)] + most_common\n",
    "    vocab = vocab[:vocab_size]\n",
    "    \n",
    "    index_to_word = [word for word, _ in vocab]\n",
    "    word_to_index = {word: idx for idx, word in enumerate(index_to_word)}\n",
    "    \n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "# Example usage\n",
    "# Assume jokes_df is your DataFrame with a column named \"joke\"\n",
    "word_to_index, index_to_word = build_vocab_from_df(jokes_df, text_column='joke', vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# Print vocabulary stats\n",
    "print(f\"Vocabulary size: {len(word_to_index)}\")\n",
    "print(\"Sample word-to-index mapping:\", dict(list(word_to_index.items())[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0\n"
     ]
    }
   ],
   "source": [
    "# only considering the 99%ile length \n",
    "joke_lengths = []\n",
    "for joke in jokes_df['joke']:\n",
    "    tokens = word_tokenizer.tokenize(joke)  \n",
    "    joke_lengths.append(len(tokens))       \n",
    "\n",
    "length_99_percentile = np.percentile(joke_lengths, 99)\n",
    "print(length_99_percentile)\n",
    "\n",
    "# removing the longer jokes\n",
    "jokes_df = jokes_df[jokes_df['joke'].apply(lambda joke: len(word_tokenizer.tokenize(joke))) <= length_99_percentile]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix :  torch.Size([30000, 512])\n",
      "Embedded example for (hello world) : tensor([[-1.8718, -1.2390, -1.1416,  ...,  0.1297,  0.8017, -0.5006],\n",
      "        [ 0.1105, -0.4442, -0.4433,  ..., -0.6023, -2.3280,  1.4897],\n",
      "        [-0.0920,  0.3739, -0.6487,  ..., -1.0594, -0.5298,  0.0730]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Embedding dimensions and vocabulary size\n",
    "embedding_dim = 512 # Example embedding dimension\n",
    "vocab_size = len(word_to_index) + 1  # Vocabulary size (from your vocab)\n",
    "\n",
    "# Define the embedding layer\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "print(\"Shape of embedding matrix : \", (embedding_layer.weight.shape))\n",
    "\n",
    "# Example encoded sequence (just using some indices from the vocabulary)\n",
    "encoded_example = torch.tensor([word_to_index.get(word, word_to_index[UNK]) for word in ['hello', 'world', 'EOS']])\n",
    "\n",
    "# Get the embeddings for the encoded sequence\n",
    "embedded_example = embedding_layer(encoded_example)\n",
    "\n",
    "print(\"Embedded example for (hello world) :\", embedded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 202, 20, 19, 45, 8, 13, 95, 384, 27, 30, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3, 5, 4121, 496, 232, 11, 27, 14, 2439, 27, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3, 7, 5501, 3509, 32, 12, 297, 218, 1132, 218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3, 18, 24, 13, 67, 11, 8894, 50, 8895, 9, 92,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3, 37, 113, 271, 27, 39, 12, 2847, 29, 113, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216322</th>\n",
       "      <td>[3, 699, 8, 426, 9928, 1, 10, 513, 2585, 20, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216323</th>\n",
       "      <td>[3, 9932, 31, 131, 26, 123, 13, 244, 163, 28, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216324</th>\n",
       "      <td>[3, 39, 24, 13, 113, 2273, 954, 35, 227, 7744,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216326</th>\n",
       "      <td>[3, 404, 59, 41, 2321, 460, 270, 5, 1125, 26, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216327</th>\n",
       "      <td>[3, 29, 44, 69, 5606, 31, 131, 9, 56, 61, 15, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208865 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     joke\n",
       "0       [3, 202, 20, 19, 45, 8, 13, 95, 384, 27, 30, 1...\n",
       "1       [3, 5, 4121, 496, 232, 11, 27, 14, 2439, 27, 2...\n",
       "2       [3, 7, 5501, 3509, 32, 12, 297, 218, 1132, 218...\n",
       "3       [3, 18, 24, 13, 67, 11, 8894, 50, 8895, 9, 92,...\n",
       "4       [3, 37, 113, 271, 27, 39, 12, 2847, 29, 113, 2...\n",
       "...                                                   ...\n",
       "216322  [3, 699, 8, 426, 9928, 1, 10, 513, 2585, 20, 1...\n",
       "216323  [3, 9932, 31, 131, 26, 123, 13, 244, 163, 28, ...\n",
       "216324  [3, 39, 24, 13, 113, 2273, 954, 35, 227, 7744,...\n",
       "216326  [3, 404, 59, 41, 2321, 460, 270, 5, 1125, 26, ...\n",
       "216327  [3, 29, 44, 69, 5606, 31, 131, 9, 56, 61, 15, ...\n",
       "\n",
       "[208865 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert jokes to token indices\n",
    "jokes_df['joke'] = jokes_df['joke'].apply(\n",
    "    lambda joke: [word_to_index.get(word, word_to_index['UNK']) for word in word_tokenizer.tokenize(joke)]\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "def pad(joke):\n",
    "    # Adding <SOS> token at the beginning\n",
    "    joke = [word_to_index['SOS']] + joke\n",
    "    # Adding <EOS> token at the end\n",
    "    joke.append(word_to_index['EOS'])\n",
    "    # Pading to match the target length\n",
    "    while len(joke) < length_99_percentile + 2:  # +2 accounts for <SOS> and <EOS>\n",
    "        joke.append(word_to_index['PAD'])\n",
    "    return joke\n",
    "\n",
    "jokes_df['joke'] = jokes_df['joke'].apply(pad)\n",
    "\n",
    "jokes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokenEmbedding(jokes, embedding_layer):\n",
    "    embedded_jokes = embedding_layer(jokes)\n",
    "    return embedded_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(joke):\n",
    "#     return [word_to_index.get(word, word_to_index['UNK']) for word in word_tokenizer.tokenize(joke)]\n",
    "\n",
    "# def pad(joke):\n",
    "#     # Adding <SOS> token at the beginning\n",
    "#     joke = [word_to_index['SOS']] + joke\n",
    "#     # Adding <EOS> token at the end\n",
    "#     joke.append(word_to_index['EOS'])\n",
    "#     # Pading to match the target length\n",
    "#     while len(joke) < length_99_percentile + 2:  # +2 accounts for <SOS> and <EOS>\n",
    "#         joke.append(word_to_index['PAD'])\n",
    "#     return joke\n",
    "\n",
    "# def TokenEmbedding(jokes, embedding_layer):\n",
    "#     embedded_jokes = embedding_layer(jokes)\n",
    "#     return embedded_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len, dropout_prob=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        max_len = int(max_len)\n",
    "\n",
    "        # Pre-compute the positional encodings\n",
    "        position = torch.arange(max_len).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim)\n",
    "        )  # Shape: (embedding_dim/2)\n",
    "\n",
    "        # Create the positional encodings\n",
    "        pe = torch.zeros(max_len, embedding_dim)  # Shape: (max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices: sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices: cos\n",
    "\n",
    "        # Add a batch dimension to the positional encodings\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, embedding_dim)\n",
    "\n",
    "        # Register the positional encodings as a buffer (non-trainable parameter)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encodings to the input embeddings\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "        # Apply dropout and return\n",
    "        return self.dropout(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_jokes shape :  torch.Size([30, 129, 512])\n",
      "positionally_encoded_jokes shape :  torch.Size([30, 129, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "jokes_tensor = torch.tensor(jokes_df['joke'].tolist(), dtype=torch.long)\n",
    "\n",
    "embedded_jokes = TokenEmbedding(jokes_tensor[:30], embedding_layer)\n",
    "print(\"embedded_jokes shape : \",embedded_jokes.shape)\n",
    "\n",
    "positional_encoding = PositionalEncoding(embedding_dim, max_len = embedded_jokes.shape[1])\n",
    "positionally_encoded_jokes = positional_encoding(embedded_jokes)\n",
    "\n",
    "print(\"positionally_encoded_jokes shape : \", positionally_encoded_jokes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Linear layers for projecting input into queries, keys, and values\n",
    "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Output linear layer to combine all heads' outputs\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Scaling factor for attention scores\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        \n",
    "        # Step 1: Linear projections for Q, K, V\n",
    "        queries = self.query_proj(x)  # Shape: (batch_size, seq_len, embed_dim)\n",
    "        keys = self.key_proj(x)       # Shape: (batch_size, seq_len, embed_dim)\n",
    "        values = self.value_proj(x)   # Shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Step 2: Reshape for multi-head attention\n",
    "        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Shapes after transpose: (batch_size, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # Step 3: Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / self.scale\n",
    "        # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Step 4: Apply causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(x.device)\n",
    "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf')).masked_fill(causal_mask == 0, 0)\n",
    "        attention_scores += causal_mask\n",
    "        \n",
    "        # Step 5: Softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Step 6: Weighted sum of values\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "        # Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # Step 7: Concatenate heads and project output\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        # Shape after transpose and reshape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        output = self.out_proj(attention_output)\n",
    "        # Shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_prob = 0.2):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # Layer Norm\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)  \n",
    "\n",
    "        # Multi Head Self Attention      \n",
    "        self.mha = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Feed Forward Network\n",
    "        self.ffn = nn.Sequential(\n",
    "        nn.Linear(embed_dim, embed_dim * 4),  # Original paper uses *4 \n",
    "        nn.GELU(),\n",
    "        nn.Linear(embed_dim * 4, embed_dim),\n",
    "        nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_mha = self.mha(self.layer_norm_1(x))\n",
    "        x = x + self.dropout(x_mha)\n",
    "        x = x + self.ffn(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, vocab_size, max_length, dropout_prob=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.position_embedding = PositionalEncoding(embedding_dim, max_length, dropout_prob)\n",
    "        \n",
    "        # Stack of Transformer Blocks\n",
    "        self.transformer_blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embedding_dim, num_heads, dropout_prob)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Linear Layer\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        x = self.position_embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        x = self.transformer_blocks(x)\n",
    "        \n",
    "        logits = self.output_layer(x)  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, jokes, max_length):\n",
    "        self.jokes = jokes\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jokes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input sequence: all tokens except the last one\n",
    "        input_seq = self.jokes[idx][:-1]\n",
    "        # Target sequence: all tokens except the first one\n",
    "        target_seq = self.jokes[idx][1:]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_data, val_data = train_test_split(jokes_df['joke'].tolist(), test_size=0.1, random_state=42)\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = length_99_percentile + 2  # Including <SOS> and <EOS>\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = JokesDataset(train_data, max_length)\n",
    "val_dataset = JokesDataset(val_data, max_length)\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 64  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "embedding_dim = 512\n",
    "vocab_size = len(word_to_index) + 1  # Including PAD\n",
    "max_length = int(max_length)\n",
    "dropout_prob = 0.4\n",
    "learning_rate = 2e-4\n",
    "num_epochs = 25  # Adjust as needed\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerDecoder(\n",
    "    num_layers=num_layers,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    vocab_size=vocab_size,\n",
    "    max_length=max_length,\n",
    "    dropout_prob=dropout_prob\n",
    ")\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function (CrossEntropyLoss ignores the PAD token by using ignore_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PAD_TOKEN])\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "# Scheduler \n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=5e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved weights\n",
    "# model.load_state_dict(torch.load(\"weights/transformer_model_6.pth\"))\n",
    "# # Move model to GPU\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what do you call a dog that can do magic ? a labracadabrador\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_sequence, max_length, k=2\n",
    "                  , temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = start_sequence\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(start_sequence)):\n",
    "            inputs = torch.tensor([generated]).to(device)\n",
    "            outputs = model(inputs)  # Shape: (1, sequence_length, vocab_size)\n",
    "\n",
    "            # Get the logits for the next token\n",
    "            logits = outputs[:, -1, :]  # Shape: (1, vocab_size)\n",
    "            logits = logits / temperature  # Apply temperature scaling\n",
    "\n",
    "            # Mask out the UNK token by setting its logit to a very negative value\n",
    "            logits[:, word_to_index[UNK]] = float('-inf')\n",
    "\n",
    "            # Get the top k tokens and their probabilities\n",
    "            top_k_probs, top_k_indices = torch.topk(torch.softmax(logits, dim=-1), k, dim=-1)  # Shape: (1, k)\n",
    "            top_k_probs = top_k_probs.squeeze(0)  # Shape: (k,)\n",
    "            top_k_indices = top_k_indices.squeeze(0)  # Shape: (k,)\n",
    "\n",
    "            # Normalize probabilities for sampling\n",
    "            normalized_probs = top_k_probs / top_k_probs.sum()\n",
    "            next_token = torch.multinomial(normalized_probs, num_samples=1).item()\n",
    "\n",
    "            # Append the sampled token\n",
    "            generated.append(top_k_indices[next_token].item())\n",
    "\n",
    "            # Stop if EOS token is generated\n",
    "            if top_k_indices[next_token].item() == word_to_index[EOS]:\n",
    "                break\n",
    "\n",
    "    # Convert generated token indices to words\n",
    "    return \" \".join([index_to_word[idx] for idx in generated if idx not in [word_to_index[EOS], word_to_index[UNK]]])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "start_sequence = [word_to_index.get(word, word_to_index[UNK]) for word in [\"what\"]]\n",
    "print(generate_text(model, start_sequence, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2622296/1040525712.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"awien/transformer_model_17.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"awien/transformer_model_17.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import math\n",
    "\n",
    "# Ensure NLTK's BLEU implementation uses tokenized references\n",
    "def compute_bleu(references, candidates):\n",
    "    # references: List of List of reference sentences (each reference is a list of tokens)\n",
    "    # candidates: List of candidate sentences (each candidate is a list of tokens)\n",
    "    return corpus_bleu(references, candidates)\n",
    "\n",
    "# Training loop\n",
    "for epoch in (range(num_epochs)):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_train_correct = 0\n",
    "    total_train_tokens = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # Shape: [batch_size, seq_len - 1, vocab_size]\n",
    "        outputs = outputs.view(-1, vocab_size)  # [batch_size * (seq_len - 1), vocab_size]\n",
    "        targets = targets.contiguous().view(-1)  # [batch_size * (seq_len - 1)]\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "        total_train_correct += (predictions == targets).sum().item()\n",
    "        total_train_tokens += targets.numel()\n",
    "\n",
    "    # Compute Training Metrics\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_perplexity = math.exp(avg_train_loss)\n",
    "    train_accuracy = total_train_correct / total_train_tokens\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total_val_correct = 0\n",
    "    total_val_tokens = 0\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation Batches\", leave=False):\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)  # Shape: [batch_size, seq_len - 1, vocab_size]\n",
    "            outputs = outputs.view(-1, vocab_size)  # [batch_size * (seq_len - 1), vocab_size]\n",
    "            targets = targets.contiguous().view(-1)  # [batch_size * (seq_len - 1)]\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            total_val_correct += (predictions == targets).sum().item()\n",
    "            total_val_tokens += targets.numel()\n",
    "\n",
    "            # Prepare for BLEU Score\n",
    "            # Reshape back to [batch_size, seq_len -1]\n",
    "            batch_size = inputs.size(0)\n",
    "            seq_len = inputs.size(1)\n",
    "\n",
    "            predictions = predictions.view(batch_size, seq_len)\n",
    "            targets = targets.view(batch_size, seq_len)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Convert indices to words, exclude PAD_TOKEN\n",
    "                pred_tokens = [index_to_word[idx] for idx in predictions[i].cpu().numpy() if idx != word_to_index[PAD_TOKEN]]\n",
    "                target_tokens = [index_to_word[idx] for idx in targets[i].cpu().numpy() if idx != word_to_index[PAD_TOKEN]]\n",
    "\n",
    "                # Append to lists\n",
    "                references.append([target_tokens])  # Each reference should be a list of references\n",
    "                candidates.append(pred_tokens)\n",
    "\n",
    "    # Compute Validation Metrics\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_perplexity = math.exp(avg_val_loss)\n",
    "    val_accuracy = total_val_correct / total_val_tokens\n",
    "    bleu_score = compute_bleu(references, candidates)\n",
    "\n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step()\n",
    "\n",
    "    # Get current LR\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "          f\"LR: {current_lr:.6f}, \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Train Perplexity: {train_perplexity:.2f}, Val Perplexity: {val_perplexity:.2f}, \"\n",
    "          f\"Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}, \"\n",
    "          f\"Val BLEU Score: {bleu_score:.4f}\")\n",
    "    start_sequence = [word_to_index.get(word, word_to_index[UNK]) for word in [\"why\"]]\n",
    "    print(generate_text(model, start_sequence, max_length))\n",
    "\n",
    "    # Save model checkpoint every 5 epochs\n",
    "    \n",
    "    torch.save(model.state_dict(), f'awien/transformer_model_{epoch + 1}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # saving model \n",
    "# torch.save(model.state_dict(), 'transformer_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what do you call a dog that likes to eat hamburgers ? labrador .\n",
      "what do a pirate and a pirate have a big ears ? a buck-an-ear\n",
      "what do you call a man with no body and no nose. nobody knows\n",
      "what do you call a man with no arms and no legs lying in front of a pile of leaves ? russell .\n",
      "what do you call a man who is n't sure if he 's alright ? a boomerang\n"
     ]
    }
   ],
   "source": [
    "def generate(model, start_sequence, max_length, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = start_sequence\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(start_sequence)):\n",
    "            inputs = torch.tensor([generated]).to(device)\n",
    "            outputs = model(inputs)  # Shape: (1, sequence_length, vocab_size)\n",
    "\n",
    "            # Get the logits for the next token\n",
    "            logits = outputs[:, -1, :]  # Shape: (1, vocab_size)\n",
    "            logits = logits / temperature  # Apply temperature scaling\n",
    "\n",
    "            # Apply a mask to the logits to exclude the UNK token\n",
    "            logits[:, word_to_index[UNK]] = float('-inf')  # Mask UNK token\n",
    "\n",
    "            # Get the top 2 tokens and their probabilities\n",
    "            top_k_probs, top_k_indices = torch.topk(torch.softmax(logits, dim=-1), k=2, dim=-1)  # Shape: (1, 2)\n",
    "            top_k_probs = top_k_probs.squeeze(0)  # Shape: (2,)\n",
    "            top_k_indices = top_k_indices.squeeze(0)  # Shape: (2,)\n",
    "\n",
    "            # Normalize probabilities for sampling\n",
    "            normalized_probs = top_k_probs / top_k_probs.sum()\n",
    "            next_token = torch.multinomial(normalized_probs, num_samples=1).item()\n",
    "\n",
    "            # Append the sampled token\n",
    "            generated.append(top_k_indices[next_token].item())\n",
    "\n",
    "            # Stop if EOS token is generated\n",
    "            if top_k_indices[next_token].item() == word_to_index[EOS]:\n",
    "                break\n",
    "\n",
    "    # Convert generated token indices to words\n",
    "    return \" \".join([index_to_word[idx] for idx in generated if idx not in [word_to_index[EOS], word_to_index[UNK]]])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "for i in range(5):\n",
    "    start_sequence = [word_to_index.get(word, word_to_index[UNK]) for word in [\"what\"]]\n",
    "    print(generate(model, start_sequence, max_length))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
