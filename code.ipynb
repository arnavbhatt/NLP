{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210957\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk \n",
    "import re\n",
    "import string\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Create DataFrame\n",
    "jokes_df = pd.read_csv(r\"reddit_dadjokes.csv\")\n",
    "\n",
    "jokes_df.drop([\"author\", \"url\", \"score\", \"date\"], axis = 1, inplace = True)\n",
    "jokes_df.drop_duplicates(\"joke\", inplace = True)\n",
    "\n",
    "# Display cleaned data information\n",
    "jokes_df.head()\n",
    "print(len(jokes_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210957\n"
     ]
    }
   ],
   "source": [
    "# cleaning the data\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "jokes_df['joke'] = jokes_df['joke'].apply(clean_text)\n",
    "\n",
    "jokes_df.head()\n",
    "print(len(jokes_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 34999\n",
      "Sample word-to-index mapping: {'PAD': 0, 'UNK': 1, 'EOS': 4, 'SOS': 3, 'a': 5, 'the': 6, 'i': 7, 'to': 8, 'my': 9, 'you': 10}\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# Constants\n",
    "UNK = 'UNK' \n",
    "PAD_TOKEN = 'PAD'\n",
    "EOS = 'EOS'\n",
    "SOS = 'SOS'\n",
    "VOCAB_SIZE = 35000  # Example vocabulary size\n",
    "\n",
    "# Initialize word tokenizer\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Function to build vocabulary\n",
    "def build_vocab_from_df(df, text_column, vocab_size):\n",
    "    words = []\n",
    "    for joke in df[text_column]:\n",
    "        tokens = word_tokenizer.tokenize(joke.lower())  # Use TreebankWordTokenizer\n",
    "        tokens.append(EOS)  # Add EOS token at the end\n",
    "        words.extend(tokens)  # Collect tokens\n",
    "    \n",
    "    # Calculate word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Get the most common words\n",
    "    most_common = word_freq.most_common(vocab_size - 4)\n",
    "    \n",
    "    # Build vocabulary with special tokens\n",
    "    vocab = [(PAD_TOKEN, 0), (UNK, 0), (EOS, 0), (SOS, 0)] + most_common\n",
    "    vocab = vocab[:vocab_size]\n",
    "    \n",
    "    index_to_word = [word for word, _ in vocab]\n",
    "    word_to_index = {word: idx for idx, word in enumerate(index_to_word)}\n",
    "    \n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "# Example usage\n",
    "# Assume jokes_df is your DataFrame with a column named \"joke\"\n",
    "word_to_index, index_to_word = build_vocab_from_df(jokes_df, text_column='joke', vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# Print vocabulary stats\n",
    "print(f\"Vocabulary size: {len(word_to_index)}\")\n",
    "print(\"Sample word-to-index mapping:\", dict(list(word_to_index.items())[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.0\n"
     ]
    }
   ],
   "source": [
    "# only considering the 97%ile length \n",
    "joke_lengths = []\n",
    "for joke in jokes_df['joke']:\n",
    "    tokens = word_tokenizer.tokenize(joke)  \n",
    "    joke_lengths.append(len(tokens))       \n",
    "\n",
    "length_97_percentile = np.percentile(joke_lengths, 97)\n",
    "print(length_97_percentile)\n",
    "\n",
    "# removing the longer jokes\n",
    "jokes_df = jokes_df[jokes_df['joke'].apply(lambda joke: len(word_tokenizer.tokenize(joke))) <= length_97_percentile]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix :  torch.Size([35000, 512])\n",
      "Embedded example for (hello world) : tensor([[ 0.3074, -1.1066, -1.5310,  ..., -0.3220,  1.8921,  1.5040],\n",
      "        [ 1.2672,  1.2303,  0.3669,  ..., -0.1957, -0.8821, -0.1679],\n",
      "        [-0.6844,  0.9205, -1.8984,  ..., -0.0276, -0.1783, -0.9040]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Embedding dimensions and vocabulary size\n",
    "embedding_dim = 512  # Example embedding dimension\n",
    "vocab_size = len(word_to_index) + 1  # Vocabulary size (from your vocab)\n",
    "\n",
    "# Define the embedding layer\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "print(\"Shape of embedding matrix : \", (embedding_layer.weight.shape))\n",
    "\n",
    "# Example encoded sequence (just using some indices from the vocabulary)\n",
    "encoded_example = torch.tensor([word_to_index.get(word, word_to_index[UNK]) for word in ['hello', 'world', 'EOS']])\n",
    "\n",
    "# Get the embeddings for the encoded sequence\n",
    "embedded_example = embedding_layer(encoded_example)\n",
    "\n",
    "print(\"Embedded example for (hello world) :\", embedded_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3, 190, 35, 170, 386, 17, 22, 10, 28, 5, 584,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3, 5, 3882, 506, 215, 8, 17, 11, 2276, 17, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3, 7, 5371, 3388, 23, 9, 272, 199, 1115, 199,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[3, 16, 20, 10, 59, 8, 8479, 40, 8281, 82, 137...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3, 31, 105, 262, 17, 26, 9, 2562, 19, 105, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216322</th>\n",
       "      <td>[3, 354, 419, 9374, 12547, 509, 2570, 59, 109,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216323</th>\n",
       "      <td>[3, 9120, 21, 114, 109, 10, 234, 145, 4, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216324</th>\n",
       "      <td>[3, 26, 20, 10, 105, 2283, 875, 25, 212, 7029,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216326</th>\n",
       "      <td>[3, 391, 49, 30, 2115, 420, 263, 5, 991, 131, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216327</th>\n",
       "      <td>[3, 19, 33, 60, 5199, 21, 114, 48, 52, 13, 19,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204857 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     joke\n",
       "0       [3, 190, 35, 170, 386, 17, 22, 10, 28, 5, 584,...\n",
       "1       [3, 5, 3882, 506, 215, 8, 17, 11, 2276, 17, 18...\n",
       "2       [3, 7, 5371, 3388, 23, 9, 272, 199, 1115, 199,...\n",
       "3       [3, 16, 20, 10, 59, 8, 8479, 40, 8281, 82, 137...\n",
       "4       [3, 31, 105, 262, 17, 26, 9, 2562, 19, 105, 19...\n",
       "...                                                   ...\n",
       "216322  [3, 354, 419, 9374, 12547, 509, 2570, 59, 109,...\n",
       "216323  [3, 9120, 21, 114, 109, 10, 234, 145, 4, 0, 0,...\n",
       "216324  [3, 26, 20, 10, 105, 2283, 875, 25, 212, 7029,...\n",
       "216326  [3, 391, 49, 30, 2115, 420, 263, 5, 991, 131, ...\n",
       "216327  [3, 19, 33, 60, 5199, 21, 114, 48, 52, 13, 19,...\n",
       "\n",
       "[204857 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert jokes to token indices\n",
    "jokes_df['joke'] = jokes_df['joke'].apply(\n",
    "    lambda joke: [word_to_index.get(word, word_to_index['UNK']) for word in word_tokenizer.tokenize(joke)]\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "def pad(joke):\n",
    "    # Adding <SOS> token at the beginning\n",
    "    joke = [word_to_index['SOS']] + joke\n",
    "    # Adding <EOS> token at the end\n",
    "    joke.append(word_to_index['EOS'])\n",
    "    # Pading to match the target length\n",
    "    while len(joke) < length_97_percentile + 2:  # +2 accounts for <SOS> and <EOS>\n",
    "        joke.append(word_to_index['PAD'])\n",
    "    return joke\n",
    "\n",
    "jokes_df['joke'] = jokes_df['joke'].apply(pad)\n",
    "\n",
    "jokes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokenEmbedding(jokes, embedding_layer):\n",
    "    embedded_jokes = embedding_layer(jokes)\n",
    "    return embedded_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(joke):\n",
    "#     return [word_to_index.get(word, word_to_index['UNK']) for word in word_tokenizer.tokenize(joke)]\n",
    "\n",
    "# def pad(joke):\n",
    "#     # Adding <SOS> token at the beginning\n",
    "#     joke = [word_to_index['SOS']] + joke\n",
    "#     # Adding <EOS> token at the end\n",
    "#     joke.append(word_to_index['EOS'])\n",
    "#     # Pading to match the target length\n",
    "#     while len(joke) < length_99_percentile + 2:  # +2 accounts for <SOS> and <EOS>\n",
    "#         joke.append(word_to_index['PAD'])\n",
    "#     return joke\n",
    "\n",
    "# def TokenEmbedding(jokes, embedding_layer):\n",
    "#     embedded_jokes = embedding_layer(jokes)\n",
    "#     return embedded_jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len, dropout_prob=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        max_len = int(max_len)\n",
    "\n",
    "        # Pre-compute the positional encodings\n",
    "        position = torch.arange(max_len).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embedding_dim, 2) * (-math.log(10000.0) / embedding_dim)\n",
    "        )  # Shape: (embedding_dim/2)\n",
    "\n",
    "        # Create the positional encodings\n",
    "        pe = torch.zeros(max_len, embedding_dim)  # Shape: (max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices: sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices: cos\n",
    "\n",
    "        # Add a batch dimension to the positional encodings\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, embedding_dim)\n",
    "\n",
    "        # Register the positional encodings as a buffer (non-trainable parameter)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encodings to the input embeddings\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "        # Apply dropout and return\n",
    "        return self.dropout(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_jokes shape :  torch.Size([30, 70, 512])\n",
      "positionally_encoded_jokes shape :  torch.Size([30, 70, 512])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "jokes_tensor = torch.tensor(jokes_df['joke'].tolist(), dtype=torch.long)\n",
    "\n",
    "embedded_jokes = TokenEmbedding(jokes_tensor[:30], embedding_layer)\n",
    "print(\"embedded_jokes shape : \",embedded_jokes.shape)\n",
    "\n",
    "positional_encoding = PositionalEncoding(embedding_dim, max_len = embedded_jokes.shape[1])\n",
    "positionally_encoded_jokes = positional_encoding(embedded_jokes)\n",
    "\n",
    "print(\"positionally_encoded_jokes shape : \", positionally_encoded_jokes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Linear layers for projecting input into queries, keys, and values\n",
    "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Output linear layer to combine all heads' outputs\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Scaling factor for attention scores\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        \n",
    "        # Step 1: Linear projections for Q, K, V\n",
    "        queries = self.query_proj(x)  # Shape: (batch_size, seq_len, embed_dim)\n",
    "        keys = self.key_proj(x)       # Shape: (batch_size, seq_len, embed_dim)\n",
    "        values = self.value_proj(x)   # Shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Step 2: Reshape for multi-head attention\n",
    "        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Shapes after transpose: (batch_size, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # Step 3: Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / self.scale\n",
    "        # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Step 4: Apply causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(x.device)\n",
    "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf')).masked_fill(causal_mask == 0, 0)\n",
    "        attention_scores += causal_mask\n",
    "        \n",
    "        # Step 5: Softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Step 6: Weighted sum of values\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "        # Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # Step 7: Concatenate heads and project output\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        # Shape after transpose and reshape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        output = self.out_proj(attention_output)\n",
    "        # Shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_prob = 0.2):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # Layer Norm\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)  \n",
    "\n",
    "        # Multi Head Self Attention      \n",
    "        self.mha = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Feed Forward Network\n",
    "        self.ffn = nn.Sequential(\n",
    "        nn.Linear(embed_dim, embed_dim * 4),  # Original paper uses *4 \n",
    "        nn.GELU(),\n",
    "        nn.Linear(embed_dim * 4, embed_dim),\n",
    "        nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_mha = self.mha(self.layer_norm_1(x))\n",
    "        x = x + self.dropout(x_mha)\n",
    "        x = x + self.ffn(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, vocab_size, max_length, dropout_prob=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.position_embedding = PositionalEncoding(embedding_dim, max_length, dropout_prob)\n",
    "        \n",
    "        # Stack of Transformer Blocks\n",
    "        self.transformer_blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embedding_dim, num_heads, dropout_prob)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final Linear Layer\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        x = self.position_embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        x = self.transformer_blocks(x)\n",
    "        \n",
    "        logits = self.output_layer(x)  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, jokes, max_length):\n",
    "        self.jokes = jokes\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jokes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input sequence: all tokens except the last one\n",
    "        input_seq = self.jokes[idx][:-1]\n",
    "        # Target sequence: all tokens except the first one\n",
    "        target_seq = self.jokes[idx][1:]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_data, val_data = train_test_split(jokes_df['joke'].tolist(), test_size=0.1, random_state=42)\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_length = length_97_percentile + 2  # Including <SOS> and <EOS>\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = JokesDataset(train_data, max_length)\n",
    "val_dataset = JokesDataset(val_data, max_length)\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 64  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10 , pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_layers = 8\n",
    "num_heads = 8\n",
    "embedding_dim = 512\n",
    "vocab_size = len(word_to_index) + 1  # Including PAD\n",
    "max_length = int(max_length)\n",
    "dropout_prob = 0.2\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 25  # Adjust as needed\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerDecoder(\n",
    "    num_layers=num_layers,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    vocab_size=vocab_size,\n",
    "    max_length=max_length,\n",
    "    dropout_prob=dropout_prob\n",
    ")\n",
    "\n",
    "def warmup_schedule(step, warmup_steps=4000, peak_lr=1e-4, d_model=512):\n",
    "    scale = peak_lr * (d_model ** -0.5)  # Scale based on model size\n",
    "    lr = scale * min((step + 1) ** -0.5, (step + 1) * (warmup_steps ** -1.5))\n",
    "    return lr\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function (CrossEntropyLoss ignores the PAD token by using ignore_index)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PAD_TOKEN])\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Scheduler \n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda step: warmup_schedule(step))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Training phase\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Unpack the batch into inputs and targets\n",
    "        inputs, targets = batch  # inputs: [batch_size, seq_len - 1], targets: [batch_size, seq_len - 1]\n",
    "        \n",
    "        # Move tensors to the appropriate device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # outputs: [batch_size, seq_len - 1, vocab_size]\n",
    "\n",
    "        # Reshape outputs and targets for loss computation\n",
    "        outputs = outputs.view(-1, vocab_size)  # [batch_size * (seq_len - 1), vocab_size]\n",
    "        targets = targets.contiguous().view(-1)  # [batch_size * (seq_len - 1)]\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update lr\n",
    "        scheduler.step()        \n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Unpack the batch into inputs and targets\n",
    "            inputs, targets = batch  # inputs: [batch_size, seq_len - 1], targets: [batch_size, seq_len - 1]\n",
    "            \n",
    "            # Move tensors to the appropriate device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # outputs: [batch_size, seq_len - 1, vocab_size]\n",
    "\n",
    "            # Reshape outputs and targets for loss computation\n",
    "            outputs = outputs.view(-1, vocab_size)  # [batch_size * (seq_len - 1), vocab_size]\n",
    "            targets = targets.contiguous().view(-1)  # [batch_size * (seq_len - 1)]\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Logging\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "    if((epoch + 1) % 5 == 0):\n",
    "        torch.save(model.state_dict(), f'transformer_model_{epoch + 1}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model \n",
    "torch.save(model.state_dict(), 'transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_sequence, max_length, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = start_sequence\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(start_sequence)):\n",
    "            inputs = torch.tensor([generated]).to(device)\n",
    "            outputs = model(inputs)  # Shape: (1, sequence_length, vocab_size)\n",
    "\n",
    "            # Get the logits for the next token\n",
    "            logits = outputs[:, -1, :]  # Shape: (1, vocab_size)\n",
    "            logits = logits / temperature  # Apply temperature scaling\n",
    "\n",
    "            # Convert logits to probabilities and sample\n",
    "            probabilities = torch.softmax(logits, dim=-1).squeeze(0)  # Shape: (vocab_size,)\n",
    "            next_token = torch.multinomial(probabilities, num_samples=1).item()\n",
    "\n",
    "            # Append the sampled token\n",
    "            generated.append(next_token)\n",
    "\n",
    "            # Stop if EOS token is generated\n",
    "            if next_token == word_to_index[EOS]:\n",
    "                break\n",
    "\n",
    "    # Convert generated token indices to words\n",
    "    return \" \".join([index_to_word[idx] for idx in generated if idx != word_to_index[PAD_TOKEN]])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "start_sequence = [word_to_index.get(word, word_to_index[UNK]) for word in [\"why\", \"did\", \"the\", \"chicken\"]]\n",
    "print(generate_text(model, start_sequence, max_length))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
